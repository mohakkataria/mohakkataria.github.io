<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Erudition]]></title><description><![CDATA[Thoughts, learnings and wisdom of a Software Engineer.]]></description><link>https://mohakkataria.github.io/</link><generator>Ghost 0.11</generator><lastBuildDate>Tue, 27 Jun 2017 12:17:26 GMT</lastBuildDate><atom:link href="https://mohakkataria.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[TCP Connection 101]]></title><description><![CDATA[A concise documentation on TCP Connection States. By : Mohak Kataria]]></description><link>https://mohakkataria.github.io/tcp-connection-101/</link><guid isPermaLink="false">0082b9a5-0f37-4eec-b9f8-31e8dd0f9600</guid><category><![CDATA[devops]]></category><category><![CDATA[tcp]]></category><category><![CDATA[zomato]]></category><dc:creator><![CDATA[Mohak Kataria]]></dc:creator><pubDate>Tue, 27 Jun 2017 11:29:46 GMT</pubDate><content:encoded><![CDATA[<p>You must have heard of TCP somewhere or the other is you have been engaged with API development and have basic knowledge as to how the code you write works on the web. But the deeper knowledge is how the data for request and the response is transferred across servers. Well Transmission Control Protocol a.k.a. TCP comes to rescue. For those well versed with <a href="https://en.wikipedia.org/wiki/OSI_model">OSI model</a>, it is a protocol for Transport Layer. Nearly every Internet-connected device “talks” TCP and the whole Internet relies on it.</p>

<h3 id="tcpconnectionstates">TCP Connection States</h3>

<p>The most common states are : <code>LISTEN</code>, <code>CLOSED</code> and <code>ESTABLISHED</code>. Most of the developers don’t know and also don't need to know more about TCP states, because this is what any application really cares about. However, a lot of stuff happens behind the scenes in the lifecycle of a TCP connection.</p>

<p>In case you want to view all the tcp connections on your system, you can use <code>netstat</code> or <code>ss</code> as :</p>

<pre><code>$ netstat -t
Active Internet connections (w/o servers)  
Proto Recv-Q Send-Q Local Address           Foreign Address         State  
tcp        0      0 localhost:49667         localhost:afs3-kaserver ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-7-233.ap-:39441 ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-1-185.ap-:54707 ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-1-25.ap-s:33442 ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-1-78.ap-s:27302 ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-7-143.ap-s:7890 ESTABLISHED  
tcp        0      0 ip-10-0-7-244.ap-:33510 ip-10-0-2-189.ap-:27017 ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-1-185.ap-:54739 ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-7-143.ap-s:7892 ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-1-78.ap-s:27118 ESTABLISHED  
tcp        0      0 ip-10-0-7-244.ap-:32902 collector-4.newre:https TIME_WAIT  
tcp        0      0 ip-10-0-7-244.ap-:57817 collector-2.newre:https TIME_WAIT  
tcp        0      0 localhost:40494         localhost:afs3-vlserver ESTABLISHED  
tcp        0      0 localhost:50019         localhost:afs3-kaserver ESTABLISHED  
tcp        0      0 ip-10-0:afs3-fileserver ip-10-0-1-78.ap-s:27304 TIME_WAIT  
...

$ss -t
State      Recv-Q Send-Q   Local Address:Port           Peer Address:Port  
ESTAB      0      0        127.0.0.1:49667              127.0.0.1:afs3-kaserver  
ESTAB      0      0        10.0.7.244:afs3-fileserver   10.0.7.233:41267  
ESTAB      0      0        10.0.7.244:afs3-fileserver   10.0.7.233:39441  
...
</code></pre>

<h5 id="connectioninitiation">Connection Initiation</h5>

<p>Connection initiation is a <mark>three-way handshake</mark>. Let's say that the party which initiates the connection is the client and the one that accepts the connection is the server.</p>

<ul>
<li>Prerequisite : A server with a listener. The listener will listen on incoming connections on a specific port. This state is represented as <code>LISTEN</code>.</li>
<li>The client sends a <code>SYN</code> packet to the server and changes it's own state to  <code>SYN-SENT</code>.</li>
<li>The server will then acknowledge the <code>SYN</code> and send a <code>SYN-ACK</code> in response to the client. </li>
<li>The client on receipt of <code>SYN-ACK</code> changes its connection state to  <code>SYN-RECEIVED</code>.</li>
<li>If everything worked properly, the client will reply with an acknowledgement <code>ACK</code> and then the connection is marked as <code>ESTABLISHED</code> on both end-points.</li>
</ul>

<p><a href="https://blog.confirm.ch/tcp-connection-states/"><img src="https://mohakkataria.github.io/content/images/2017/06/imgonline-com-ua-progressiveh5h0pu3pFm4g.jpg" alt="" title=""></a><sup id="fnref:1"><a href="https://mohakkataria.github.io/tcp-connection-101/#fn:1" rel="footnote">1</a></sup></p>

<h5 id="datatransfer">Data Transfer</h5>

<p>Now the data transfer can happen between client and the server. Each party sends some data in packets and the other party responds with acknowledgement <code>ACK</code> for the data packet received.</p>

<h5 id="connectiontermination">Connection Termination</h5>

<p>Either of client or server can terminate the connection if they are finished with the data exchange or timeout happens or variety of other reasons. However, it can not be dropped because their other end-point needs to know about the termination. Terminating a connection is a <mark>four-way handshake</mark>. And because each end-point is terminating the connection independently. It doesn’t matter which end-point starts the termination, because both of them will change their states accordingly with the information being sent to the other party about termination. Let's assume the client starts the termination.  </p>

<p><a href="https://blog.confirm.ch/tcp-connection-states/"><img src="https://mohakkataria.github.io/content/images/2017/06/imgonline-com-ua-progressiveAYXURwLHrBFb.jpg" alt="" title=""></a><sup id="fnref:1"><a href="https://mohakkataria.github.io/tcp-connection-101/#fn:1" rel="footnote">1</a></sup></p>

<ul>
<li>The client sends a <code>FIN</code> packet to the server and changes its state to  <code>FIN-WAIT-1</code>.</li>
<li>The server receives the termination request from the client and responds with an acknowledgement <code>ACK</code>.</li>
<li>After sending the response, the server will change its state to <code>CLOSE-WAIT</code> state.</li>
<li>As soon as the client receives this acknowledgement from the server, it will go to the <code>FIN-WAIT-2</code> state.</li>
</ul>

<p>In the above process, the connection is terminated from a client point of view, the server is yet to terminate its connection. This happens right after the server sent its last <code>ACK</code>.</p>

<ul>
<li>The server is in the <code>CLOSE-WAIT</code> state and it will independently follow up with a <code>FIN</code>, and updates it's state to <code>LAST-ACK</code>.</li>
<li>Now the client receives the termination request and replies with an  <code>ACK</code>, which results in a <code>TIME-WAIT</code> state.</li>
<li>The server is now finished and changes it's connection state to <code>CLOSED</code> immediately.</li>
<li>The client stays in the <code>TIME-WAIT</code> state for a maximum of four minutes (defined by RFC793 and the maximum segment lifetime, read further for more details) before setting the connection to <code>CLOSED</code> as well.</li>
</ul>

<h5 id="timewait">TIME-WAIT ???</h5>

<p>Now all this is fairly easy but there might be a question in mind that why is <code>TIME-WAIT</code> necessary ? If both client and server are terminating the connection mutually and gracefully, why does <code>TIME-WAIT</code> exist ?</p>

<p>The purpose of TIME-WAIT is to prevent delayed packets from one connection being accepted by a later connection. Concurrent connections are isolated by other mechanisms, primarily by addresses, ports, and sequence numbers. When a duplicate packet from the first connection is delayed in the network and arrives at the second connection when its sequence number is in the second connection’s window, there is no way for the endpoints in the second connection to determine that the delayed packet contains data from the first connection. The situation is shown below : </p>

<p><img src="https://mohakkataria.github.io/content/images/2017/06/fig1.svg" alt=""></p>

<ul>
<li>TCP avoids this condition by blocking any second connection between these address/port pairs until one can assume that all duplicates must have disappeared. </li>
<li>Connection blocking is implemented by holding a <code>TIME-WAIT TCB(TCP Control Block)</code> at one endpoint and checking incoming connection requests to ensure that no new connection is established between the blocked addresses and ports. </li>
<li>The TCB is held for twice the <code>Maximum Segment Lifetime (MSL)</code>. The MSL is defined as the longest period of time that a packet can remain undelivered in the network.</li>
<li>Originally, the TTL field of an IP packet was the amount of time the packet could remain undelivered, but in practice the field has become a hop count. </li>
<li>Therefore, the MSL is an estimate rather than a guarantee. Under most conditions waiting 2 x MSL is sufficient to drain duplicates, but they can and do arrive after that time. The chance of a duplicate arriving after 2 x MSL is greater if MSL is smaller.</li>
</ul>

<p>Check <a href="https://tools.ietf.org/html/draft-faber-time-wait-avoidance-00">this</a> out for more details about the effects of TIME-WAIT on busy servers, persistent connections and avoidance techniques.</p>

<p>I can go into more detail but that tends to be a topic of research in the field of Computer Networks and Communications. Moreover, this article is supposed to act as prequel to an article cum case study on the <code>Effects of TCP states on production traffic</code> as observed during my time at <code>Zomato</code>. So keep tuned for more!!! :)</p>

<hr>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p><a href="https://blog.confirm.ch/tcp-connection-states/">https://blog.confirm.ch/tcp-connection-states/</a> <a href="https://mohakkataria.github.io/tcp-connection-101/#fnref:1" title="return to article">↩</a></p></li></ol></div>]]></content:encoded></item><item><title><![CDATA[Rate Limiting with Nginx]]></title><description><![CDATA[Insight into how nginx can be used for API/web level rate limiting. 
By : Mohak Kataria]]></description><link>https://mohakkataria.github.io/rate-limiting-with-nginx/</link><guid isPermaLink="false">1155d120-4bc3-4724-9d1d-6c2b545ac85e</guid><category><![CDATA[nginx]]></category><category><![CDATA[devops]]></category><category><![CDATA[api]]></category><dc:creator><![CDATA[Mohak Kataria]]></dc:creator><pubDate>Sat, 10 Jun 2017 17:23:00 GMT</pubDate><media:content url="https://mohakkataria.github.io/content/images/2017/06/kibana42_apache.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://mohakkataria.github.io/content/images/2017/06/kibana42_apache.jpg" alt="Rate Limiting with Nginx"><p>Now that you have landed at this article after reading the headline, I assume that you already know what Nginx is and why is it used in front of our backend servers/services as a reverse proxying agent. Well, in case you still landed at this article without that knowledge, fret not as I am attaching a small list of resources which can help you with that :</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Reverse_proxy">Wikipedia - Reverse Proxy</a></li>
<li><a href="https://www.nginx.com/resources/glossary/reverse-proxy-server/">Nginx Docs - Reverse Proxy</a></li>
<li><a href="http://www.jscape.com/blog/bid/87783/Forward-Proxy-vs-Reverse-Proxy">Forward v/s Reverse Proxy</a></li>
</ul>

<p>So now we are at a place where we can start with the real deal of learning as to how can one use nginx for rate limiting the traffic to your website/API. This will also work even when using behind a load balancer or when nginx is being used as one, till the actual IPs are passed in headers to nginx for it to differentiate between requests from different hosts. It is useful, if your site is hammered by a bot doing multiple requests per second and thus increasing your server load. With the <code>ngx_http_limit_req_module</code> you can define a rate limit, and if a visitor exceeds this rate, he will get a 503 error. The rate limiting is performed via "<a href="https://en.wikipedia.org/wiki/Leaky_bucket">Leaky Bucket</a>" algorithm usually employed in computer networks with bandwidth limitations. </p>

<p>To start with, we first define a <code>limit_req_zone</code> in our nginx.conf as shown below. Now if you ask why is that necessary, the answer to that would be, Computer Science &amp; Logic 101 : We need to solve a problem of limiting traffic to an IP based on the traffic, but we need to know what the traffic is corresponding to every IP and then compute on the data basis our business need/logic as to when to limit the resources to that IP.</p>

<pre><code class="language-http">    http {
        limit_req_zone $binary_remote_addr zone=test:20m rate=10r/s;
        ...

        server {
            ...
</code></pre>

<p>This sets the shared memory zone with the requisite rate of requests. Here the shared memory zone is called test and is allocated 20MB of storage. Instead of the variable <code>$remote_addr</code>, we use the variable <code>$binary_remote_addr</code> which reduces the size of the state to 64 bytes. There can be about 16,000 states in a 1MB zone, so 20MB allow for about 320,000 states, so this should be enough for your visitors, but you may change it depending on how much traffic you receive. The rate is limited to ten request per second(rps). Please note that rps must be an integer values. So half a request per second should be set as 30 rps. This configuration of setting the request zone must go inside the <code>http {}</code> container.</p>

<p>After having defined a storage area to store the data, we use this to actually put rate limiting to use. This is done using the <code>limit_req</code> directive. One can use this directive in <code>http {}</code>, <code>server {}</code>, and <code>location {}</code> containers, but it is most useful in <code>location {}</code> containers that pass requests to your app servers (PHP-FPM, etc.) because otherwise, if you load a single page with lots of assets (images, CSS, and JavaScript files), you would probably exceed the given rate limit with a single page request.</p>

<p>An example directive usage is as </p>

<pre><code>        location ~ \.php$ {
                ...
                limit_req zone=test burst=10;
                ...
        }
</code></pre>

<p><br>
<code>limit_req zone=test burst=10;</code> specifies that this rate limit belongs to the session storage area we defined before in the nginx.conf. Burst is nothing but a queue which means that if you exceed the rate limit, the following requests are delayed, and only if you have more requests waiting in the queue than specified in the burst parameter, will you get a 503 error like the image shown below or any other html page which has been defined in configuration or a default one by nginx :</p>

<figure>  
<img src="https://mohakkataria.github.io/content/images/2017/06/litespeed-503-error.gif" alt="Rate Limiting with Nginx">
<figcaption>Sample 503 Error Page</figcaption>  
</figure>

<p>When rate limited, the nginx error logs will produce output similar to the following:</p>

<pre>
2016/10/20 17:28:46 [error] 30347#0: *55 limiting requests, excess: 5.658 by zone "test", client: 10.170.2.13, server: www.example.com, request: "GET /test/results/?keyword= HTTP/1.1", host: "test-site-www.example.com", referrer: "https://test-site-www.example.com/test/results/?keyword=" 
2016/10/20 17:28:46 [error] 30347#0: *55 limiting requests, excess: 5.273 by zone "test", client: 10.170.2.13, server: www.example.com, request: "GET /test/results/?keyword= HTTP/1.1", host: "test-site-www.example.com", referrer: "https://test-site-www.example.com/test/results/?keyword=" 
2016/10/20 17:28:47 [error] 30347#0: *55 limiting requests, excess: 5.508 by zone "test", client: 10.170.2.13, server: www.example.com, request: "GET /test/results/?keyword= HTTP/1.1", host: "test-site-www.example.com", referrer: "https://test-site-www.example.com/test/results/?keyword=" 
2016/10/20 17:28:47 [error] 30347#0: *55 limiting requests, excess: 5.200 by zone "test", client: 10.170.2.13, server: www.example.com, request: "GET /test/results/?keyword= HTTP/1.1", host: "test-site-www.example.com", referrer: "https://test-site-www.example.com/test/results/?keyword=" 
2016/10/20 17:28:48 [error] 30347#0: *55 limiting requests, excess: 5.567 by zone "test", client: 10.170.2.13, server: www.example.com, request: "GET /test/results/?keyword= HTTP/1.1", host: "test-site-www.example.com", referrer: "https://test-site-www.example.com/search/results/?keyword=" 
</pre>

<p>With this basic configuration, we have enabled rate limiting on our service. Wasn't it too simple? :)</p>

<div class="tip">  
PS : Don't forget to reload nginx after adding or making changes to configuration files for them to take effect using command :  
`sudo service nginx reload` or similar on your OS.
</div>]]></content:encoded></item></channel></rss>
